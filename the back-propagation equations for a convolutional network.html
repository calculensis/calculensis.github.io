<!DOCTYPE html>
<html lang="en">
<head>
          <title>the decision blog - the back-propagation equations for a convolutional network</title>
        <meta charset="utf-8" />
        <meta name="generator" content="Pelican" />
        <link href="https://calculensis.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="the decision blog Full Atom Feed" />
        <link href="https://calculensis.github.io/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="the decision blog Full RSS Feed" />
        <link href="https://calculensis.github.io/feeds/machine-learning.atom.xml" type="application/atom+xml" rel="alternate" title="the decision blog Categories Atom Feed" />




    <meta name="tags" content="back-propagation" />
    <meta name="tags" content="convolutional network" />

</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://calculensis.github.io/">the decision blog</a></h1>
        </header><!-- /#banner -->
        <nav id="menu"><ul>
            <li class="active"><a href="https://calculensis.github.io/category/machine-learning.html">machine learning</a></li>
            <li><a href="https://calculensis.github.io/category/productivity.html">productivity</a></li>
            <li><a href="https://calculensis.github.io/category/simple-tools.html">simple tools</a></li>
            <li><a href="https://calculensis.github.io/category/sql.html">SQL</a></li>
            <li><a href="https://calculensis.github.io/category/systems-thinking.html">systems thinking</a></li>
        </ul></nav><!-- /#menu -->
<section id="content" class="body">
  <header>
    <h2 class="entry-title">
      <a href="https://calculensis.github.io/the back-propagation equations for a convolutional network.html" rel="bookmark"
         title="Permalink to the back-propagation equations for a convolutional network">the back-propagation equations for a convolutional network</a></h2>
 
  </header>
  <footer class="post-info">
    <time class="published" datetime="2023-03-01T00:00:00-05:00">
      Wed 01 March 2023
    </time>
    <address class="vcard author">
      By           <a class="url fn" href="https://calculensis.github.io/author/kayla-lewis.html">Kayla Lewis</a>
    </address>
    <div class="category">
        Category: <a href="https://calculensis.github.io/category/machine-learning.html">machine learning</a>
    </div>
    <div class="tags">
        Tags:
            <a href="https://calculensis.github.io/tag/back-propagation.html">back-propagation</a>
            <a href="https://calculensis.github.io/tag/convolutional-network.html">convolutional network</a>
    </div>
  </footer><!-- /.post-info -->
  <div class="entry-content">
    <p><img align=right src="images/blurred_mirror.jpg" width="150"/></p>
<p>In this post, we'll derive the back-propagation equations for our convolutional net, which has the structure shown below. The square blocks refer to 2D matrices, while the rectangles represent matrices with a single row. They are drawn vertically, however, to make the diagram cleaner.</p>
<p><img src="images/conv_nn_struct.jpg" width="300"/></p>
<table>
<thead>
<tr>
<th>matrix</th>
<th>size</th>
<th>meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="math">\(\textbf{k}^i\)</span></td>
<td><span class="math">\(1\times n^k\)</span></td>
<td>i<span class="math">\(^{th}\)</span> kernel</td>
</tr>
<tr>
<td><span class="math">\(\textbf{F}\)</span></td>
<td><span class="math">\(n^k \times n^0\)</span></td>
<td>image frame captures</td>
</tr>
<tr>
<td><span class="math">\(\textbf{l}^{ci}\)</span></td>
<td><span class="math">\(1\times n^0\)</span></td>
<td>i<span class="math">\(^{th}\)</span> pre-pooled layer</td>
</tr>
<tr>
<td><span class="math">\(\textbf{l}^0\)</span></td>
<td><span class="math">\(1\times n^0\)</span></td>
<td>pooled convolution layer</td>
</tr>
<tr>
<td><span class="math">\(\textbf{w}^{01}\)</span></td>
<td><span class="math">\(n^0 \times n^1\)</span></td>
<td>weights from layer 0 to 1</td>
</tr>
<tr>
<td><span class="math">\(\textbf{l}^1\)</span></td>
<td><span class="math">\(1\times n^1\)</span></td>
<td>hidden layer</td>
</tr>
<tr>
<td><span class="math">\(\textbf{w}^{12}\)</span></td>
<td><span class="math">\(n^1 \times n^2\)</span></td>
<td>weights from layer 1 to 2</td>
</tr>
<tr>
<td><span class="math">\(\textbf{l}^2\)</span></td>
<td><span class="math">\(1\times n^2\)</span></td>
<td>output layer</td>
</tr>
</tbody>
</table>
<p>In terms of components, the convolutional layer is given by
</p>
<div class="math">$$
l_i^{cj}=\sum_m k_m^j F_{mi},
$$</div>
<p>
and
</p>
<div class="math">$$
l_k^0 = \tanh \left[ \max \left(l_k^{c0},l_k^{c1},...,l_k^{cN}\right) \right].
$$</div>
<p>
For layers 1 and 2 we have
</p>
<div class="math">$$
l_q^1=\tanh \left[\sum_r l_r^0 w_{rq}^{01}\right].
$$</div>
<p>
and
</p>
<div class="math">$$
l_n^2 = \sigma \left[\sum_l l_l^1 w_{ln}^{12} \right],
$$</div>
<p>
where <span class="math">\(\sigma\)</span> is the softmax function. The loss function is
</p>
<div class="math">$$
\epsilon = \frac{1}{2}\sum_p \left(l_p^2-y_p \right)^2,
$$</div>
<p>
and by applying the chain rule to these equations we will derive the back-propagation equations.</p>
<p><img src="images/variable_depend.jpg" width="150"/></p>
<p>Similarly to the 3 layer non-convolutional net, we have
</p>
<div class="math">$$
\frac{\partial \epsilon}{\partial w_{qs}^{12}}=\left[\textbf{l}^1\otimes\left(\textbf{l}^2-\textbf{y}\right)\textbf{D}_{s'}\right]_{qs}
$$</div>
<p>
and
</p>
<div class="math">$$
\frac{\partial \epsilon}{\partial w_{qs}^{01}}=\left[\textbf{l}^0 \otimes \left(\textbf{l}^2-\textbf{y}\right)\textbf{D}_{\sigma'}\textbf{w}^{12,T}\textbf{D}_{t'}^1\right]_{qs}.
$$</div>
<p>
but now in addition to these we have
</p>
<div class="math">$$
\frac{\partial \epsilon}{\partial k_q^i}=\sum_{n,m,p}\frac{\partial \epsilon}{\partial l_m^2}\frac{\partial l_m^2}{\partial l_n^1}\frac{\partial l_n^1}{\partial l_p^0}\frac{\partial l_p^0}{\partial k_q^i}.
$$</div>
<p>
These derivatives are
</p>
<div class="math">$$
\frac{\partial \epsilon}{\partial l_m^2}=\sum_p \left(l_p^2-y_p\right)\delta_{pm}=\left(l_m^2-y_m\right),
$$</div>
<div class="math">$$
\frac{\partial \l_m^2}{\partial l_n^1}=\sigma_m'\sum_l w_{lm}^{12}\delta_{ln}=\sigma_m' w_{nm}^{12},
$$</div>
<div class="math">$$
\frac{\partial l_n^1}{\partial l_p^0}=t_n'\sum_r \delta_{pr}w_{rn}^{01}=t_n'w_{pn}^{01},
$$</div>
<p>
and
</p>
<div class="math">$$
\frac{\partial l_p^0}{\partial k_q^i}=\frac{\partial}{\partial k_q^i}\{t\left[\max(l_p^{c0},l_p^{c1},...,l_p^{cN})\right]\}=t_p^{0'}\frac{\partial l_p^{c,w(p)}}{\partial k_q^i},
$$</div>
<p>
where <span class="math">\(w(p)\)</span> is the index of the largest <span class="math">\(l_p^{ci}\)</span> ("w" is for "winner"). This last derivative is given by
</p>
<div class="math">$$
\frac{\partial l_p^{c,w(p)}}{\partial k_q^i}=\frac{\partial}{\partial k_q^i}\left[\sum_m k_m^{w(p)}F_{mp}\right]=\sum_m F_{mp}\delta^{i,w(p)}\delta_{mq}=F_{qp}\delta^{i,w(p)}.
$$</div>
<p>
Defining
</p>
<div class="math">$$
\mathcal{F}_{qp}^i\equiv F_{qp}\delta^{i,w(p)},
$$</div>
<p>
this result can be written more compactly as
</p>
<div class="math">$$
\frac{\partial l_p^{c,w(p)}}{\partial k_q^i}=\mathcal{F}_{qp}^i,
$$</div>
<p>
so that
</p>
<div class="math">$$
\frac{\partial l_p^0}{\partial k_q^i}=t_p^{0'}\mathcal{F}_{qp}^i.
$$</div>
<p>
Hence, for the components of the gradient corresponding to the kernels, we obtain
</p>
<div class="math">$$
\frac{\partial \epsilon}{\partial k_{q}^i}=\sum_{n,m,p}\left(l_m^2-y_m\right)\sigma_m' w_{mn}^{12,T}t_n^{1'} w_{np}^{01,T}t_p^{0'}\mathcal{F}_{pq}^{i,T},
$$</div>
<p>
which can be written as
</p>
<div class="math">$$
\frac{\partial \epsilon}{\partial k_{q}^i}=\left[\left(\textbf{l}^2-\textbf{y}\right)\textbf{D}_{\sigma'}\textbf{w}^{12,T}\textbf{D}_{t'}^1\textbf{w}^{01,T}\textbf{D}_{t'}^0 \mathcal{F}^{i,T} \right]_q.
$$</div>
<p><a href="https://twitter.com/Estimatrix/status/1555693184977600512?s=20&amp;t=YFPoxpEQ2Qp14U4FliD7fA">Discuss on Twitter</a></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div><!-- /.entry-content -->
</section>
        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>,
                which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
</body>
</html>